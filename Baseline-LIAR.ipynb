{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOL9GmEOtryLXd9YqKlVsCd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Data loading (LIAR)"],"metadata":{"id":"zzB52lQv03B2"}},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset = load_dataset(\"liar\")"],"metadata":{"id":"KM0LxnGxz8hx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","import pickle\n","\n","X_train = dataset['train']['statement']\n","X_val = dataset['validation']['statement']\n","X_test = dataset['test']['statement']\n","\n","y_train = dataset['train']['label']\n","y_val = dataset['validation']['label']\n","y_test = dataset['test']['label']"],"metadata":{"id":"PhSxDTc_z81o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BASELINE 1"],"metadata":{"id":"1GOIAHRzXwNL"}},{"cell_type":"code","source":["total_full_stop_train = []\n","\n","for article in X_train:\n","    full_stop_count = 0\n","    for word in article:\n","        full_stop_count += word.count('.')\n","    total_full_stop_train.append(full_stop_count)"],"metadata":{"id":"Pgjs15q4WfQr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dat_size = len(total_full_stop_train)\n","pos = sum(total_full_stop_train[:int(dat_size/2)]) / (dat_size/2)\n","neg = sum(total_full_stop_train[int(dat_size/2):]) / (dat_size/2)\n","print(pos)\n","print(neg)\n","boundary = (pos + neg) / 2\n","print(boundary)"],"metadata":{"id":"IXWklgcCavrz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_full_stop_val = []\n","\n","for article in X_val:\n","    full_stop_count = 0\n","    for word in str(article):\n","        full_stop_count += word.count('.')\n","    total_full_stop_val.append(full_stop_count)"],"metadata":{"id":"PT2lJN6NlNAh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","predictions = [0 if x > boundary else 1 for x in total_full_stop_val]\n","\n","acc = accuracy_score(y_val, predictions)\n","\n","print('Accuracy: ', acc)"],"metadata":{"id":"ewGx422PlORx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_full_stop_test = []\n","\n","for article in X_test:\n","    full_stop_count = 0\n","    for word in str(article):\n","        full_stop_count += word.count('.')\n","    total_full_stop_test.append(full_stop_count)"],"metadata":{"id":"AqPW3yp3e8bz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = [0 if x > boundary else 1 for x in total_full_stop_test]\n","\n","acc = accuracy_score(y_test, predictions)\n","\n","print('Accuracy: ', acc)"],"metadata":{"id":"gUyDhHt8b5kI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BASELINE 2"],"metadata":{"id":"_PvZcyMvWfiO"}},{"cell_type":"code","source":["import gensim\n","import gensim.downloader\n","\n","word2vec = gensim.downloader.load('word2vec-google-news-300')"],"metadata":{"id":"FA622ZF3Wcyz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","import numpy as np\n","\n","def score(model, word):\n","    try:\n","        return model[word]\n","    except KeyError:\n","        # Handle the case when one or both words are not in the vocabulary\n","        return np.zeros(300)\n","\n","Train_Data = []\n","\n","max_words = 10\n","print(\"Processing train articles ...\")\n","# Iterate over each article\n","for article in X_train:\n","    # Create a matrix for the current article\n","    article_matrix = [score(word2vec, word) for word in article.split()[:10]]\n","    # Add the matrix to the list\n","    padding_size = max_words - len(article_matrix)\n","    if padding_size > 0:\n","        article_matrix += [np.zeros(300)] * padding_size\n","    Train_Data.append(np.asarray(article_matrix).flatten())\n","\n","\n","Train_Data = np.array(Train_Data)\n","\n","print(\"Articles processed!\")\n","\n","model = LogisticRegression(max_iter=1000)\n","\n","model.fit(Train_Data, y_train)"],"metadata":{"id":"A2YFsdhOhG6U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Val_Data = []\n","print(\"Processing val articles ...\")\n","# Iterate over each article\n","for article in X_val:\n","    # Create a matrix for the current article\n","    article_matrix = [score(word2vec, word) for word in article.split()[:10]]\n","    # Add the matrix to the list\n","\n","    padding_size = max_words - len(article_matrix)\n","    if padding_size > 0:\n","        article_matrix += [np.zeros(300)] * padding_size\n","    Val_Data.append(np.asarray(article_matrix).flatten())\n","\n","Val_Data = np.array(Val_Data)\n","\n","print(\"Articles processed!\")\n","\n","predictions = model.predict(Val_Data)\n","\n","\n","acc = accuracy_score(y_val, predictions)\n","\n","\n","print('Scores for Baseline 2:')\n","print('Accuracy: ', acc)"],"metadata":{"id":"nq4eJC8hj3Pl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Test_Data = []\n","print(\"Processing test articles ...\")\n","# Iterate over each article\n","for article in X_test:\n","    # Create a matrix for the current article\n","    article_matrix = [score(word2vec, word) for word in article.split()[:10]]\n","    # Add the matrix to the list\n","    padding_size = max_words - len(article_matrix)\n","    if padding_size > 0:\n","        article_matrix += [np.zeros(300)] * padding_size\n","\n","    Test_Data.append(np.asarray(article_matrix).flatten())\n","\n","Test_Data = np.array(Test_Data)\n","\n","print(\"Articles processed!\")\n","\n","predictions = model.predict(Test_Data)\n","\n","\n","acc = accuracy_score(y_test, predictions)\n","\n","print('Scores for Baseline 2:')\n","print('Accuracy: ', acc)"],"metadata":{"id":"1MX536gUlC0Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ebV0fQqCJiwX"},"execution_count":null,"outputs":[]}]}