{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTOnly(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTOnly, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(.2)\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.dense_1 = nn.Linear(768, 128)\n",
    "\n",
    "        self.dense_2 = nn.Linear(128, 2)\n",
    "\n",
    "\n",
    "    def forward(self, tokens, mask):\n",
    "\n",
    "        x = self.bert(tokens, attention_mask=mask)['last_hidden_state'][:, 0]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.softmax(self.dense_2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeBERT, self).__init__()\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.conv1d_1 = nn.Conv1d(100, 128, kernel_size=3)\n",
    "        self.conv1d_2 = nn.Conv1d(100, 128, kernel_size=4)\n",
    "        self.conv1d_3 = nn.Conv1d(100, 128, kernel_size=5)\n",
    "\n",
    "        self.maxpool_1 = nn.MaxPool1d(kernel_size=5)\n",
    "        self.maxpool_2 = nn.MaxPool1d(kernel_size=5)\n",
    "        self.maxpool_3 = nn.MaxPool1d(kernel_size=5)\n",
    "\n",
    "        self.conv1d_4 = nn.Conv1d(128, 128, kernel_size=5)\n",
    "        self.maxpool_4 = nn.MaxPool1d(kernel_size=5)\n",
    "\n",
    "        self.conv1d_5 = nn.Conv1d(128, 128, kernel_size=5)\n",
    "        self.maxpool_5 = nn.MaxPool1d(kernel_size=28)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.dense_1 = nn.Linear(384, 128)\n",
    "        self.dense_2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, tokens, mask):\n",
    "\n",
    "        x = self.bert(tokens, attention_mask=mask)['last_hidden_state']\n",
    "\n",
    "        x1 = F.relu(self.conv1d_1(x))\n",
    "        x2 = F.relu(self.conv1d_2(x))\n",
    "        x3 = F.relu(self.conv1d_3(x))\n",
    "\n",
    "        x1 = self.maxpool_1(x1)\n",
    "        x2 = self.maxpool_2(x2)\n",
    "        x3 = self.maxpool_3(x3)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=2)\n",
    "\n",
    "\n",
    "        x = F.relu(self.conv1d_4(x))\n",
    "        x = self.maxpool_4(x)\n",
    "\n",
    "        x = F.relu(self.conv1d_5(x))\n",
    "        x = self.maxpool_5(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.softmax(self.dense_2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTwithLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTwithLSTM, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(.6)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.LSTM = nn.LSTM(768, 256, 1, batch_first=True)\n",
    "\n",
    "        self.ff_1 = nn.Linear(256, 128)\n",
    "\n",
    "        self.BatchNorm = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.ff_2 = nn.Linear(128, 32)\n",
    "\n",
    "        self.ff_3 = nn.Linear(32, 2)\n",
    "\n",
    "\n",
    "    def forward(self, tokens, mask):\n",
    "\n",
    "        x = self.bert(tokens, attention_mask=mask)['last_hidden_state'][:, 0]\n",
    "\n",
    "        output, _ = self.LSTM(x)\n",
    "\n",
    "        x = F.relu(self.ff_1(output))\n",
    "\n",
    "        x = self.BatchNorm(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.ff_2(x))\n",
    "\n",
    "        x = F.softmax(self.ff_3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTOurModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTOurModel, self).__init__()\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(.6)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.LSTM = nn.LSTM(768, 256, 2, batch_first=True)\n",
    "\n",
    "        self.ff_1 = nn.Linear(256, 128)\n",
    "\n",
    "        self.BatchNorm = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.ff_2 = nn.Linear(128, 32)\n",
    "\n",
    "        self.ff_3 = nn.Linear(32, 2)\n",
    "\n",
    "\n",
    "    def forward(self, tokens, mask):\n",
    "\n",
    "        x = self.bert(tokens, attention_mask=mask)['last_hidden_state']\n",
    "\n",
    "        output, _ = self.LSTM(x)\n",
    "        out = output[:, -1, :]\n",
    "\n",
    "        x = F.relu(self.ff_1(out))\n",
    "\n",
    "        x = self.BatchNorm(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.ff_2(x))\n",
    "\n",
    "        x = F.softmax(self.ff_3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading (O'Brien et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load data\n",
    "real_examples = pickle.load(open('./clean/real.pkl', \"rb\"))\n",
    "real_examples = [s.strip() for s in real_examples]\n",
    "real_labels = [1] * len(real_examples)\n",
    "\n",
    "fake_examples = pickle.load(open('./clean/fake.pkl', \"rb\"))\n",
    "fake_examples = [s.strip() for s in fake_examples]\n",
    "fake_labels = [0] * len(fake_examples)\n",
    "\n",
    "X = real_examples + fake_examples\n",
    "y = real_labels + fake_labels\n",
    "\n",
    "X_trainval, X_testb, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_trainb, X_valb, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = tokenizer.batch_encode_plus(\n",
    "    X_trainb,\n",
    "    max_length = 100,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "train_ids = torch.tensor(X_train['input_ids'])\n",
    "train_mask = torch.tensor(X_train['attention_mask'])\n",
    "train_label = torch.tensor(y_train)\n",
    "\n",
    "X_val = tokenizer.batch_encode_plus(\n",
    "    X_valb,\n",
    "    max_length = 100,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "val_ids = torch.tensor(X_val['input_ids'])\n",
    "val_mask = torch.tensor(X_val['attention_mask'])\n",
    "val_label = torch.tensor(y_val)\n",
    "\n",
    "X_test = tokenizer.batch_encode_plus(\n",
    "    X_testb,\n",
    "    max_length = 100,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "test_ids = torch.tensor(X_test['input_ids'])\n",
    "test_mask = torch.tensor(X_test['attention_mask'])\n",
    "test_label = torch.tensor(y_test)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_mask, train_label)\n",
    "val_dataset = TensorDataset(val_ids, val_mask, val_label)\n",
    "test_dataset = TensorDataset(test_ids, test_mask, test_label)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model to use and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTwithLSTM().to(device)\n",
    "model_save_path = \"bertlstm.pt\"\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=1e-2)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    print(f\"Beginning epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "\n",
    "    for inputs, attention_mask, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, attention_mask)\n",
    "\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        train_correct += torch.sum(predictions == labels)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_accuracy = train_correct.double() / len(train_dataset)\n",
    "    train_loss /= len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_mask, labels in val_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, attention_mask)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(predictions == labels)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_accuracy = val_correct.double() / len(val_dataset)\n",
    "    val_loss /= len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get metrics on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def get_scores(model, test_dataloader):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_mask, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, attention_mask)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            predictions = predictions.tolist()\n",
    "            all_predictions += predictions\n",
    "            labels = labels.tolist()\n",
    "            all_labels += labels\n",
    "\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_predictions)\n",
    "    prec = precision_score(all_labels, all_predictions)\n",
    "    rec = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "    return acc, prec, rec, f1, all_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_model = BERTwithLSTM()\n",
    "current_model.load_state_dict(torch.load('bertlstm.pt'))\n",
    "current_model.cuda()\n",
    "current_model.eval()\n",
    "\n",
    "acc, prec, rec, f1, l, pr = get_scores(current_model, test_dataloader)\n",
    "\n",
    "print('Accuracy: ', acc)\n",
    "print('Precision: ', prec)\n",
    "print('Recall: ', rec)\n",
    "print('F1 Score: ', f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
