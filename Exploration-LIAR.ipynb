{"cells":[{"cell_type":"markdown","metadata":{"id":"DJL-BItx3YmW"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vikFolUF3Yme"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import BertModel, BertTokenizer\n","from sklearn.model_selection import train_test_split\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"apjCX25Y3Ymh"},"source":["## Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYslTxHZ3Ymh"},"outputs":[],"source":["class BERTOnly(nn.Module):\n","    def __init__(self):\n","        super(BERTOnly, self).__init__()\n","\n","        self.dropout = nn.Dropout(.2)\n","\n","        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n","\n","        self.dense_1 = nn.Linear(768, 128)\n","\n","        self.dense_2 = nn.Linear(128, 6)\n","\n","\n","    def forward(self, tokens, mask):\n","\n","        x = self.bert(tokens, attention_mask=mask)['last_hidden_state'][:, 0]\n","\n","        x = self.dropout(x)\n","        x = F.relu(self.dense_1(x))\n","        x = self.dropout(x)\n","        x = F.softmax(self.dense_2(x))\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1uNfyguE3Ymi"},"outputs":[],"source":["class FakeBERT(nn.Module):\n","    def __init__(self):\n","        super(FakeBERT, self).__init__()\n","\n","\n","        self.dropout = nn.Dropout(.2)\n","        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n","\n","        self.conv1d_1 = nn.Conv1d(100, 128, kernel_size=3)\n","        self.conv1d_2 = nn.Conv1d(100, 128, kernel_size=4)\n","        self.conv1d_3 = nn.Conv1d(100, 128, kernel_size=5)\n","\n","        self.maxpool_1 = nn.MaxPool1d(kernel_size=5)\n","        self.maxpool_2 = nn.MaxPool1d(kernel_size=5)\n","        self.maxpool_3 = nn.MaxPool1d(kernel_size=5)\n","\n","        self.conv1d_4 = nn.Conv1d(128, 128, kernel_size=5)\n","        self.maxpool_4 = nn.MaxPool1d(kernel_size=5)\n","\n","        self.conv1d_5 = nn.Conv1d(128, 128, kernel_size=5)\n","        self.maxpool_5 = nn.MaxPool1d(kernel_size=28)\n","\n","        self.flatten = nn.Flatten()\n","\n","        self.dense_1 = nn.Linear(384, 128)\n","        self.dense_2 = nn.Linear(128, 6)\n","\n","    def forward(self, tokens, mask):\n","\n","        x = self.bert(tokens, attention_mask=mask)['last_hidden_state']\n","\n","        x1 = F.relu(self.conv1d_1(x))\n","        x2 = F.relu(self.conv1d_2(x))\n","        x3 = F.relu(self.conv1d_3(x))\n","\n","        x1 = self.maxpool_1(x1)\n","        x2 = self.maxpool_2(x2)\n","        x3 = self.maxpool_3(x3)\n","\n","        x = torch.cat((x1, x2, x3), dim=2)\n","\n","\n","        x = F.relu(self.conv1d_4(x))\n","        x = self.maxpool_4(x)\n","\n","        x = F.relu(self.conv1d_5(x))\n","        x = self.maxpool_5(x)\n","\n","        x = self.flatten(x)\n","\n","        x = self.dropout(x)\n","        x = F.relu(self.dense_1(x))\n","        x = self.dropout(x)\n","        x = F.softmax(self.dense_2(x))\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7T-JYbX53Ymj"},"outputs":[],"source":["class BERTwithLSTM(nn.Module):\n","    def __init__(self):\n","        super(BERTwithLSTM, self).__init__()\n","\n","        self.dropout = nn.Dropout(.6)\n","        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n","\n","        self.LSTM = nn.LSTM(768, 256, 1, batch_first=True)\n","\n","        self.ff_1 = nn.Linear(256, 128)\n","\n","        self.BatchNorm = nn.BatchNorm1d(128)\n","\n","        self.ff_2 = nn.Linear(128, 32)\n","\n","        self.ff_3 = nn.Linear(32, 6)\n","\n","\n","    def forward(self, tokens, mask):\n","\n","        x = self.bert(tokens, attention_mask=mask)['last_hidden_state'][:, 0]\n","\n","        output, _ = self.LSTM(x)\n","\n","        x = F.relu(self.ff_1(output))\n","\n","        x = self.BatchNorm(x)\n","\n","        x = self.dropout(x)\n","\n","        x = F.relu(self.ff_2(x))\n","\n","        x = F.softmax(self.ff_3(x))\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaOlMw9p3Yml"},"outputs":[],"source":["class BERTOurModel(nn.Module):\n","    def __init__(self):\n","        super(BERTOurModel, self).__init__()\n","\n","\n","        self.dropout = nn.Dropout(.6)\n","        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n","\n","        self.LSTM = nn.LSTM(768, 256, 2, batch_first=True)\n","\n","        self.ff_1 = nn.Linear(256, 128)\n","\n","        self.BatchNorm = nn.BatchNorm1d(128)\n","\n","        self.ff_2 = nn.Linear(128, 32)\n","\n","        self.ff_3 = nn.Linear(32, 6)\n","\n","\n","    def forward(self, tokens, mask):\n","\n","        x = self.bert(tokens, attention_mask=mask)['last_hidden_state']\n","\n","        output, _ = self.LSTM(x)\n","        out = output[:, -1, :]\n","\n","        x = F.relu(self.ff_1(out))\n","\n","        x = self.BatchNorm(x)\n","\n","        x = self.dropout(x)\n","\n","        x = F.relu(self.ff_2(x))\n","\n","        x = F.softmax(self.ff_3(x))\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"4ywbtoSq3Ymm"},"source":["## Data loading (LIAR)"]},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset = load_dataset(\"liar\")"],"metadata":{"id":"5ZY2UGvA4E4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DISn1ViV3Ymn"},"outputs":[],"source":["from torch.utils.data import DataLoader, TensorDataset\n","from transformers import BertTokenizer, BertModel\n","from sklearn.model_selection import train_test_split\n","import pickle\n","\n","\n","X_trainb = dataset['train']['statement']\n","X_valb = dataset['validation']['statement']\n","X_testb = dataset['test']['statement']\n","\n","y_train = dataset['train']['label']\n","y_val = dataset['validation']['label']\n","y_test = dataset['test']['label']\n","\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","print(f\"Tokenizing data...\")\n","X_train = tokenizer.batch_encode_plus(\n","    X_trainb,\n","    max_length = 100,\n","    padding='max_length',\n","    truncation=True\n",")\n","\n","train_ids = torch.tensor(X_train['input_ids'])\n","train_mask = torch.tensor(X_train['attention_mask'])\n","train_label = F.one_hot(torch.tensor(y_train))\n","\n","X_val = tokenizer.batch_encode_plus(\n","    X_valb,\n","    max_length = 100,\n","    padding='max_length',\n","    truncation=True\n",")\n","\n","val_ids = torch.tensor(X_val['input_ids'])\n","val_mask = torch.tensor(X_val['attention_mask'])\n","val_label = F.one_hot(torch.tensor(y_val))\n","\n","X_test = tokenizer.batch_encode_plus(\n","    X_testb,\n","    max_length = 100,\n","    padding='max_length',\n","    truncation=True\n",")\n","\n","test_ids = torch.tensor(X_test['input_ids'])\n","test_mask = torch.tensor(X_test['attention_mask'])\n","test_label = F.one_hot(torch.tensor(y_test))\n","\n","train_dataset = TensorDataset(train_ids, train_mask, train_label)\n","val_dataset = TensorDataset(val_ids, val_mask, val_label)\n","test_dataset = TensorDataset(test_ids, test_mask, test_label)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=8)\n","test_dataloader = DataLoader(test_dataset, batch_size=8)"]},{"cell_type":"markdown","metadata":{"id":"pfUdpd573Ymo"},"source":["## Set model to use and hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YDl1t003Ymo"},"outputs":[],"source":["model = BERTwithLSTM().to(device)\n","model_save_path = \"bertlstm.pt\"\n","optimizer = torch.optim.Adadelta(model.parameters(), lr=1e-2)\n","num_epochs = 10"]},{"cell_type":"markdown","metadata":{"id":"iVgw68PC3Ymo"},"source":["## Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGCGBzxt3Ymo"},"outputs":[],"source":["# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define the optimizer and loss function\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training loop\n","best_val_loss = float(\"inf\")\n","\n","for epoch in range(num_epochs):\n","    # Training\n","    print(f\"Beginning epoch {epoch+1}/{num_epochs}\")\n","    model.train()\n","    train_loss = 0.0\n","    train_correct = 0\n","\n","    for inputs, attention_mask, labels in train_dataloader:\n","        inputs = inputs.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs, attention_mask)\n","\n","        _, predictions = torch.max(outputs, 1)\n","        _, labels = torch.max(labels, 1)\n","        train_correct += torch.sum(predictions == labels)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item() * inputs.size(0)\n","\n","    train_accuracy = train_correct.double() / len(train_dataset)\n","    train_loss /= len(train_dataset)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    val_correct = 0\n","\n","    with torch.no_grad():\n","        for inputs, attention_mask, labels in val_dataloader:\n","            inputs = inputs.to(device)\n","            attention_mask = attention_mask.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs, attention_mask)\n","\n","            _, predictions = torch.max(outputs, 1)\n","            _, labels = torch.max(labels, 1)\n","            val_correct += torch.sum(predictions == labels)\n","\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item() * inputs.size(0)\n","\n","    val_accuracy = val_correct.double() / len(val_dataset)\n","    val_loss /= len(val_dataset)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}\")\n","    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n","    print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n","    print(\"-----------------------------------------\")\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), model_save_path)"]},{"cell_type":"markdown","metadata":{"id":"oQdvvlqs3Ymp"},"source":["## Get metrics on test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QrEmp95H3Ymp"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","def get_scores(model, test_dataloader):\n","    all_predictions = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for inputs, attention_mask, labels in test_dataloader:\n","            inputs = inputs.to(device)\n","            attention_mask = attention_mask.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs, attention_mask)\n","\n","            _, predictions = torch.max(outputs, 1)\n","            _, labels = torch.max(labels, 1)\n","            predictions = predictions.tolist()\n","            all_predictions += predictions\n","            labels = labels.tolist()\n","            all_labels += labels\n","\n","\n","    acc = accuracy_score(all_labels, all_predictions)\n","\n","    return acc, all_labels, all_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPM9XFPe3Ymq"},"outputs":[],"source":["current_model = BERTwithLSTM()\n","current_model.load_state_dict(torch.load('bertlstm.pt'))\n","current_model.cuda()\n","current_model.eval()\n","\n","acc, l, pr = get_scores(current_model, test_dataloader)\n","\n","print('Accuracy: ', acc)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.7"},"colab":{"provenance":[{"file_id":"1GmzWjBuYWdXYkPW0XZ0pUHWPBEOZFJmV","timestamp":1686795877015}]}},"nbformat":4,"nbformat_minor":0}